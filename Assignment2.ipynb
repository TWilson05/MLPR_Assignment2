{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.linalg import cho_factor, cho_solve\n",
    "data = np.load('DATA/ct_data.npz')\n",
    "X_train = data['X_train']; X_val = data['X_val']; X_test = data['X_test']\n",
    "y_train = data['y_train']; y_val = data['y_val']; y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (40754, 384)\n",
      "y_train shape: (40754,)\n"
     ]
    }
   ],
   "source": [
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean on y_train is -0.0.\n",
      "The mean in y_val is -0.21600851 +/- 0.01290338.\n",
      "The mean on the first 5785 entries in y_train is -0.44247688 +/- 0.01192627.\n"
     ]
    }
   ],
   "source": [
    "#defining a function to calculate standard error\n",
    "def cal_se(data):\n",
    "    std = np.std(data)\n",
    "    se = std/ (len(data)**(1/2))\n",
    "    return se\n",
    "\n",
    "#verifying the mean on the training set is zero\n",
    "print(f'The mean on y_train is {round(np.mean(y_train), 8)}.')\n",
    "print(f'The mean in y_val is {round(np.mean(y_val),8)} +/- {round(cal_se(y_val), 8)}.')\n",
    "\n",
    "print(f'The mean on the first 5785 entries in y_train is {round(np.mean(y_train[:5785]), 8)} +/- {round(cal_se(y_train[:5785]),8)}.')\n",
    "#used 8 decimal places as that is the pattern of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard errors on the mean values of the first 5785 entries of training and the validation dataset suggest that even the edge cases of the mean of the two sets does not meet the overal population mean values. The standard error bars are misleading here because they do not align to what the population mean is??? \n",
    "\n",
    "standard error --> makes it look like we are more certain than we are. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40754, 373) (5785, 373) (6961, 373)\n",
      "constant columns: [ 59  69 179 189 351]\n",
      "duplicate columns: [ 69  78  79 179 188 189 199 287 351 359]\n"
     ]
    }
   ],
   "source": [
    "isConstant = []\n",
    "isDuplicates = np.array(np.zeros(X_train.shape[1]), dtype='bool') \n",
    "\n",
    "#identify all the constant columns\n",
    "for i in range(X_train.shape[1]) :\n",
    "    isConstant.append((X_train[:,i] == X_train[0][i]).all())\n",
    "\n",
    "#identify all the columns which are duplicates to previous columns\n",
    "for i in range(X_train.shape[1]) :\n",
    "    for k in range(i+1, X_train.shape[1]) :\n",
    "        if isDuplicates[k] == False:\n",
    "            isDuplicates[k] = (X_train[:,i] == X_train[:,k]).all()\n",
    "\n",
    "#create a list with all the columns to remove\n",
    "columns_to_remove = np.unique(np.hstack((np.nonzero(isDuplicates)[0], np.nonzero(isConstant)[0])))\n",
    "\n",
    "#remove these columns and name them modified datasets\n",
    "X_train = np.delete(X_train, columns_to_remove ,1)\n",
    "X_val = np.delete(X_val, columns_to_remove ,1)\n",
    "X_test = np.delete(X_test, columns_to_remove ,1)\n",
    "\n",
    "#check if the shape still aligns to expectations (number of rows stay the same, number of columns are same for all three sets)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "# print list of removed columns\n",
    "print(f'constant columns: {np.nonzero(isConstant)[0]}')\n",
    "print(f'duplicate columns: {np.nonzero(isDuplicates)[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to set up the following matrix equation to solve for the weights and bias of the linear regression model.\n",
    "$$ \\tilde{\\Phi}=\\left[\\begin{matrix} X_{N\\times D} & 1_{N\\times 1}\\\\ \\sqrt{\\alpha}\\mathbb{I}_{D\\times D} & 0_{D\\times 1} \\end{matrix}\\right],\\quad \\underline{\\tilde{w}}=\\left[\\begin{matrix} w_{D\\times 1}\\\\ b\\end{matrix}\\right],\\quad \\underline{\\tilde{y}}=\\left[\\begin{matrix} y_{D\\times 1}\\\\ 0\\end{matrix}\\right] $$\n",
    "$$ E(\\underline{w},b)=(\\tilde{\\Phi}\\underline{\\tilde{w}}-\\underline{\\tilde{y}})^T(\\tilde{\\Phi}\\underline{\\tilde{w}}-\\underline{\\tilde{y}}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linreg(X, yy, alpha):\n",
    "    yy = yy[:, np.newaxis]\n",
    "    # add a column of ones to the X matrix\n",
    "    Phi = np.hstack((X, np.ones((X.shape[0],1))))\n",
    "    # add an identity matrix to the Phi matrix for regularization\n",
    "    # leave the last column as zeros to ignore the bias term\n",
    "    Phi_til = np.vstack((Phi, np.hstack((np.sqrt(alpha)*np.eye(X.shape[1]), np.zeros((X.shape[1],1))))))\n",
    "    # compute the new y vector\n",
    "    yy_til = np.vstack((yy, np.zeros((X.shape[1],1))))\n",
    "    # compute the weights\n",
    "    w = np.linalg.lstsq(Phi_til, yy_til, rcond=None)[0][:,0]\n",
    "    return w[:-1], w[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the weighs and bias for the linreg model\n",
    "w_linreg, b_linreg = fit_linreg(X_train, y_train, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ct_support_code import fit_linreg_gradopt\n",
    "# compute the weighs and bias for the linreg_gradopt model\n",
    "w_grad, b_grad = fit_linreg_gradopt(X_train, y_train, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(X, yy, ww, bb):\n",
    "    residuals = X @ ww[:, np.newaxis] + bb - yy[:, np.newaxis]\n",
    "    return np.sqrt(residuals.T @ residuals / len(yy))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training linear regression: 0.3567565397204048\n",
      "RMSE for training gradient descent: 0.3567556103401207\n",
      "RMSE for validation linear regression: 0.4230521968394693\n",
      "RMSE for validation gradient descent: 0.4230551058620388\n"
     ]
    }
   ],
   "source": [
    "print(f'RMSE for training linear regression: {compute_rmse(X_train, y_train, w_linreg, b_linreg)}')\n",
    "print(f'RMSE for training gradient descent: {compute_rmse(X_train, y_train, w_grad, b_grad)}')\n",
    "print(f'RMSE for validation linear regression: {compute_rmse(X_val, y_val, w_linreg, b_linreg)}')\n",
    "print(f'RMSE for validation gradient descent: {compute_rmse(X_val, y_val, w_grad, b_grad)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ct_support_code import logreg_cost, minimize_list\n",
    "#write a function to fit logistc regresion using gradient opt\n",
    "def fit_logreg_gradopt(X, yy, alpha):\n",
    "    D = X.shape[1]\n",
    "    args = (X, yy, alpha)\n",
    "    init = (np.zeros(D), np.array(0))\n",
    "    ww, bb = minimize_list(logreg_cost, init, args)\n",
    "    return ww, bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 20 # number of thresholded classification problems to fit\n",
    "mx = np.max(y_train); mn = np.min(y_train); hh = (mx-mn)/(K+1)\n",
    "thresholds = np.linspace(mn+hh, mx-hh, num=K, endpoint=True)\n",
    "params = []\n",
    "for kk in range(K):\n",
    "    labels = y_train > thresholds[kk]\n",
    "    # ... fit logistic regression to these labels\n",
    "    params.append(fit_logreg_gradopt(X_train, labels, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list_train = []\n",
    "pred_list_val = []\n",
    "\n",
    "#sigmoid function to be used to normalise X\n",
    "def sigmoid_func(x): \n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "#utilise the parameters from the model fitted earlier and apply sigmoid on the predictions\n",
    "for i in range(K):\n",
    "    pred_list_train.append(sigmoid_func(X_train @ params[i][0] + params[i][1]))\n",
    "    pred_list_val.append(sigmoid_func(X_val @ params[i][0] + params[i][1]))\n",
    "    \n",
    "X_train_transform = np.vstack(pred_list_train).T\n",
    "X_val_transform = np.vstack(pred_list_val).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit linear regression on the predictions\n",
    "w_train, b_train = fit_linreg(X_train_transform, y_train, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training linear regression: 0.15441150429956377\n",
      "RMSE for training linear regression: 0.25424772979325594\n"
     ]
    }
   ],
   "source": [
    "print(f'RMSE for training linear regression: {compute_rmse(X_train_transform, y_train, w_train, b_train)}')\n",
    "print(f'RMSE for training linear regression: {compute_rmse(X_val_transform, y_val, w_train, b_train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting a neural networks with pre-determined weights \n",
    "from ct_support_code import nn_cost\n",
    "def fit_nn(init, X, yy, alpha):\n",
    "    args = (X, yy, alpha)\n",
    "    ww_bar, bb_bar, V_bar, bk_bar = minimize_list(nn_cost, init, args)\n",
    "    return ww_bar, bb_bar, V_bar, bk_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jaimecheung/Documents/University/0MLPR/MLPR_Assignment2/Assignment2.ipynb Cell 25\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jaimecheung/Documents/University/0MLPR/MLPR_Assignment2/Assignment2.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m input_biases \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([params[k][\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(K)])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jaimecheung/Documents/University/0MLPR/MLPR_Assignment2/Assignment2.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m init \u001b[39m=\u001b[39m w_train, b_train, input_weights, input_biases\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jaimecheung/Documents/University/0MLPR/MLPR_Assignment2/Assignment2.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m new_params \u001b[39m=\u001b[39m fit_nn(init, X_train, y_train, \u001b[39m30\u001b[39m)\n",
      "\u001b[1;32m/Users/jaimecheung/Documents/University/0MLPR/MLPR_Assignment2/Assignment2.ipynb Cell 25\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jaimecheung/Documents/University/0MLPR/MLPR_Assignment2/Assignment2.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_nn\u001b[39m(init, X, yy, alpha):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jaimecheung/Documents/University/0MLPR/MLPR_Assignment2/Assignment2.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     args \u001b[39m=\u001b[39m (X, yy, alpha)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jaimecheung/Documents/University/0MLPR/MLPR_Assignment2/Assignment2.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     ww_bar, bb_bar, V_bar, bk_bar \u001b[39m=\u001b[39m minimize_list(nn_cost, init, args)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jaimecheung/Documents/University/0MLPR/MLPR_Assignment2/Assignment2.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m ww_bar, bb_bar, V_bar, bk_bar\n",
      "File \u001b[0;32m~/Documents/University/0MLPR/MLPR_Assignment2/ct_support_code.py:58\u001b[0m, in \u001b[0;36mminimize_list\u001b[0;34m(cost, init_list, args)\u001b[0m\n\u001b[1;32m     56\u001b[0m     vec_bar, _ \u001b[39m=\u001b[39m params_wrap(params_bar)\n\u001b[1;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m E, vec_bar\n\u001b[0;32m---> 58\u001b[0m res \u001b[39m=\u001b[39m minimize(wrap_cost, init, args, \u001b[39m'\u001b[39m\u001b[39mL-BFGS-B\u001b[39m\u001b[39m'\u001b[39m, jac\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, options\u001b[39m=\u001b[39mopt)\n\u001b[1;32m     59\u001b[0m \u001b[39mreturn\u001b[39;00m unwrap(res\u001b[39m.\u001b[39mx)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/optimize/_minimize.py:696\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    693\u001b[0m     res \u001b[39m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    694\u001b[0m                              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[1;32m    695\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ml-bfgs-b\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 696\u001b[0m     res \u001b[39m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[1;32m    697\u001b[0m                            callback\u001b[39m=\u001b[39mcallback, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[1;32m    698\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtnc\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    699\u001b[0m     res \u001b[39m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[39m=\u001b[39mcallback,\n\u001b[1;32m    700\u001b[0m                         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/optimize/_lbfgsb_py.py:359\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    353\u001b[0m task_str \u001b[39m=\u001b[39m task\u001b[39m.\u001b[39mtobytes()\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFG\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[1;32m    356\u001b[0m     \u001b[39m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[39m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     \u001b[39m# Overwrite f and g:\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     f, g \u001b[39m=\u001b[39m func_and_grad(x)\n\u001b[1;32m    360\u001b[0m \u001b[39melif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNEW_X\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    361\u001b[0m     \u001b[39m# new iteration\u001b[39;00m\n\u001b[1;32m    362\u001b[0m     n_iterations \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:285\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39marray_equal(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx):\n\u001b[1;32m    284\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_x_impl(x)\n\u001b[0;32m--> 285\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fun()\n\u001b[1;32m    286\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_grad()\n\u001b[1;32m    287\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_fun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    250\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated:\n\u001b[0;32m--> 251\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fun_impl()\n\u001b[1;32m    252\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_fun\u001b[39m():\n\u001b[0;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[39m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[39m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m fx \u001b[39m=\u001b[39m fun(np\u001b[39m.\u001b[39mcopy(x), \u001b[39m*\u001b[39margs)\n\u001b[1;32m    138\u001b[0m \u001b[39m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misscalar(fx):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/optimize/_optimize.py:76\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39margs):\n\u001b[1;32m     75\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_if_needed(x, \u001b[39m*\u001b[39margs)\n\u001b[1;32m     77\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/optimize/_optimize.py:70\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mall(x \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(x)\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m---> 70\u001b[0m     fg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfun(x, \u001b[39m*\u001b[39margs)\n\u001b[1;32m     71\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39m=\u001b[39m fg[\u001b[39m1\u001b[39m]\n\u001b[1;32m     72\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39m=\u001b[39m fg[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/University/0MLPR/MLPR_Assignment2/ct_support_code.py:55\u001b[0m, in \u001b[0;36mminimize_list.<locals>.wrap_cost\u001b[0;34m(vec, *args)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_cost\u001b[39m(vec, \u001b[39m*\u001b[39margs):\n\u001b[0;32m---> 55\u001b[0m     E, params_bar \u001b[39m=\u001b[39m cost(unwrap(vec), \u001b[39m*\u001b[39margs)\n\u001b[1;32m     56\u001b[0m     vec_bar, _ \u001b[39m=\u001b[39m params_wrap(params_bar)\n\u001b[1;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m E, vec_bar\n",
      "File \u001b[0;32m~/Documents/University/0MLPR/MLPR_Assignment2/ct_support_code.py:197\u001b[0m, in \u001b[0;36mnn_cost\u001b[0;34m(params, X, yy, alpha)\u001b[0m\n\u001b[1;32m    195\u001b[0m P_bar \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(F_bar[:,\u001b[39mNone\u001b[39;00m], ww[\u001b[39mNone\u001b[39;00m,:]) \u001b[39m# N,K\u001b[39;00m\n\u001b[1;32m    196\u001b[0m A_bar \u001b[39m=\u001b[39m P_bar \u001b[39m*\u001b[39m P \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m P) \u001b[39m# N,K\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m V_bar \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(A_bar\u001b[39m.\u001b[39mT, X) \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m\u001b[39m*\u001b[39malpha\u001b[39m*\u001b[39mV \u001b[39m# K,D\u001b[39;00m\n\u001b[1;32m    198\u001b[0m bk_bar \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(A_bar, \u001b[39m0\u001b[39m)\n\u001b[1;32m    200\u001b[0m \u001b[39mreturn\u001b[39;00m E, (ww_bar, bb_bar, V_bar, bk_bar)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_weights = np.vstack([params[k][0] for k in range(K)])\n",
    "input_biases = np.array([params[k][1] for k in range(K)])\n",
    "init = w_train, b_train, input_weights, input_biases\n",
    "\n",
    "new_params = fit_nn(init, X_train, y_train, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting a neural networks with random weights\n",
    "\n",
    "rand_hidden_weights = np.random.uniform(-1, 1, K)\n",
    "rand_hidden_bias = np.random.uniform(-1, 1)\n",
    "rand_input_weights = np.random.uniform(-1, 1, (K, X_train.shape[1]))\n",
    "rand_input_bias = np.random.uniform(-1, 1, K)\n",
    "rand_init = rand_hidden_weights, rand_hidden_bias, rand_input_weights, rand_input_bias\n",
    "\n",
    "rand_nn_params = fit_nn(rand_init, X_train, y_train, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for train NN: 0.13962174633354246\n",
      "RMSE for val NN: 0.2684062894655103\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "nn_train_predictions = nn_cost(new_params, X_train)\n",
    "nn_val_predictions = nn_cost(new_params, X_val)\n",
    "def nn_rmse(predictions, yy):\n",
    "    residuals = (predictions - yy)[:,np.newaxis]\n",
    "    return np.sqrt(residuals.T @ residuals / len(yy))[0][0]\n",
    "print(f'RMSE for train NN: {nn_rmse(nn_train_predictions, y_train)}')\n",
    "print(f'RMSE for val NN: {nn_rmse(nn_val_predictions, y_val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for train NN: 0.13930023727216795\n",
      "RMSE for val NN: 0.2719298622850644\n"
     ]
    }
   ],
   "source": [
    "rand_nn_train_predictions = nn_cost(rand_nn_params, X_train)\n",
    "rand_nn_val_predictions = nn_cost(rand_nn_params, X_val)\n",
    "print(f'RMSE for train NN: {nn_rmse(rand_nn_train_predictions, y_train)}')\n",
    "print(f'RMSE for val NN: {nn_rmse(rand_nn_val_predictions, y_val)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn_reg(alpha, X_train, y_train, X_val, y_val):\n",
    "    hidden_weights = np.random.uniform(-1, 1, K)\n",
    "    hidden_bias = np.random.uniform(-1, 1)\n",
    "    input_weights = np.random.uniform(-1, 1, (K, X_train.shape[1]))\n",
    "    input_bias = np.random.uniform(-1, 1, K)\n",
    "    init = hidden_weights, hidden_bias, input_weights, input_bias\n",
    "\n",
    "    nn_params = fit_nn(init, X_train, y_train, alpha)\n",
    "    val_predictions = nn_cost(nn_params, X_val)\n",
    "    return nn_rmse(val_predictions, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = np.array([0,25,50])\n",
    "baseline = np.log(nn_rmse(rand_nn_val_predictions, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_nn_rmse = np.array([train_nn_reg(alpha, X_train, y_train, X_val, y_val) for alpha in alpha_list])\n",
    "alpha_nn_y_vals = baseline - np.log(alpha_nn_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ct_support_code import gp_post_par\n",
    "alpha_all = np.arange(0, 50, 0.02)\n",
    "alpha_rest = np.setdiff1d(alpha_all, alpha_list)\n",
    "gp_post_par_resutlts = gp_post_par(alpha_rest, alpha_list, alpha_nn_y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "def acquisition_function(gp_post_par_resutlts, y_obs):\n",
    "    return stats.norm.cdf((gp_post_par_resutlts[0] - np.max(y_obs))/np.diag(gp_post_par_resutlts[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = np.array([0,25,50])\n",
    "alpha_nn_rmse = alpha_nn_rmse[:3]\n",
    "alpha_nn_y_vals = alpha_nn_y_vals[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.66\n",
      "12.64\n",
      "9.14\n",
      "36.42\n"
     ]
    }
   ],
   "source": [
    "new_alphas = []\n",
    "prob_imp_list = []\n",
    "for i in range(5):\n",
    "    prob_imp = acquisition_function(gp_post_par_resutlts, alpha_nn_y_vals)\n",
    "    prob_imp_list.append(prob_imp[np.argmax(prob_imp)])\n",
    "    new_alpha = alpha_all[np.argmax(prob_imp)]\n",
    "    new_alphas.append(new_alpha)\n",
    "    print(new_alpha)\n",
    "    if i != 4:\n",
    "        alpha_list = np.append(alpha_list, new_alpha)\n",
    "        alpha_nn_rmse = np.append(alpha_nn_rmse, train_nn_reg(new_alpha, X_train, y_train, X_val, y_val))\n",
    "        alpha_nn_y_vals = baseline - np.log(alpha_nn_rmse)\n",
    "        gp_post_par_resutlts = gp_post_par(alpha_rest, alpha_list, alpha_nn_y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3113905214879238, 0.07923897635238492, 0.003945606495911536, 3.3341753499797556e-14, 1.7024546985393874e-23]\n",
      "[20.68, 16.66, 12.64, 9.14, 36.42]\n"
     ]
    }
   ],
   "source": [
    "print(prob_imp_list)\n",
    "print(new_alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.68 0.3113905214879238\n"
     ]
    }
   ],
   "source": [
    "best_index = np.argmax(prob_imp_list)\n",
    "best_prob_imp = prob_imp_list[best_index]\n",
    "best_alpha = new_alphas[best_index]\n",
    "print(best_alpha, best_prob_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for train NN: 0.12106856533283024\n",
      "RMSE for val NN: 0.26104825630741324\n"
     ]
    }
   ],
   "source": [
    "def get_nn_RMSE(alpha, X_train, y_train, X_val, y_val):\n",
    "    hidden_weights = np.random.uniform(-1, 1, K)\n",
    "    hidden_bias = np.random.uniform(-1, 1)\n",
    "    input_weights = np.random.uniform(-1, 1, (K, X_train.shape[1]))\n",
    "    input_bias = np.random.uniform(-1, 1, K)\n",
    "    init = hidden_weights, hidden_bias, input_weights, input_bias\n",
    "\n",
    "    nn_params = fit_nn(init, X_train, y_train, alpha)\n",
    "    train_predictions = nn_cost(nn_params, X_train)\n",
    "    val_predictions = nn_cost(nn_params, X_val)\n",
    "    print(f'RMSE for train NN: {nn_rmse(train_predictions, y_train)}')\n",
    "    print(f'RMSE for val NN: {nn_rmse(val_predictions, y_val)}')\n",
    "\n",
    "get_nn_RMSE(best_alpha, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training linear regression using K = 10: 0.16535613879572222\n",
      "RMSE for validation linear regression using K = 10: 0.25517932306696745\n",
      "RMSE for training linear regression using K = 11: 0.16227092552526698\n",
      "RMSE for validation linear regression using K = 11: 0.2562588770565986\n",
      "RMSE for training linear regression using K = 12: 0.16124738352291107\n",
      "RMSE for validation linear regression using K = 12: 0.2634884269861179\n",
      "RMSE for training linear regression using K = 13: 0.1599645656189831\n",
      "RMSE for validation linear regression using K = 13: 0.2532120922733342\n",
      "RMSE for training linear regression using K = 14: 0.15734860991637672\n",
      "RMSE for validation linear regression using K = 14: 0.24867525239682137\n",
      "RMSE for training linear regression using K = 15: 0.15637190445431332\n",
      "RMSE for validation linear regression using K = 15: 0.25795910081294854\n",
      "RMSE for training linear regression using K = 16: 0.15510273762278062\n",
      "RMSE for validation linear regression using K = 16: 0.2583045195255074\n",
      "RMSE for training linear regression using K = 17: 0.15597554996025667\n",
      "RMSE for validation linear regression using K = 17: 0.2508178666136102\n",
      "RMSE for training linear regression using K = 18: 0.15433223366857352\n",
      "RMSE for validation linear regression using K = 18: 0.25425056778294564\n",
      "RMSE for training linear regression using K = 19: 0.15447101942097108\n",
      "RMSE for validation linear regression using K = 19: 0.25573686036800114\n",
      "RMSE for training linear regression using K = 20: 0.15401495629185952\n",
      "RMSE for validation linear regression using K = 20: 0.2535972483423365\n",
      "RMSE for training linear regression using K = 21: 0.15429567723161464\n",
      "RMSE for validation linear regression using K = 21: 0.25427366669645923\n",
      "RMSE for training linear regression using K = 22: 0.15379960444242452\n",
      "RMSE for validation linear regression using K = 22: 0.25311571833109914\n",
      "RMSE for training linear regression using K = 23: 0.15376121436450849\n",
      "RMSE for validation linear regression using K = 23: 0.2542701027781262\n",
      "RMSE for training linear regression using K = 24: 0.15293646624599308\n",
      "RMSE for validation linear regression using K = 24: 0.25342255756415955\n",
      "RMSE for training linear regression using K = 25: 0.15315923004344761\n",
      "RMSE for validation linear regression using K = 25: 0.2535055470024505\n",
      "RMSE for training linear regression using K = 26: 0.1531086294241153\n",
      "RMSE for validation linear regression using K = 26: 0.2547709204761237\n",
      "RMSE for training linear regression using K = 27: 0.15240841059008378\n",
      "RMSE for validation linear regression using K = 27: 0.2518466377269294\n",
      "RMSE for training linear regression using K = 28: 0.15295724684423873\n",
      "RMSE for validation linear regression using K = 28: 0.2538741052277105\n",
      "RMSE for training linear regression using K = 29: 0.15280245587131241\n",
      "RMSE for validation linear regression using K = 29: 0.25344598916699895\n",
      "RMSE for training linear regression using K = 30: 0.15243422771155918\n",
      "RMSE for validation linear regression using K = 30: 0.2542559532926265\n"
     ]
    }
   ],
   "source": [
    "# tunning the best k \n",
    "K_list = np.arange(10,31)\n",
    "k_tunning_results = []\n",
    "\n",
    "for K in K_list:\n",
    "\n",
    "    mx = np.max(y_train); mn = np.min(y_train); hh = (mx-mn)/(K+1)\n",
    "    thresholds = np.linspace(mn+hh, mx-hh, num=K, endpoint=True)\n",
    "    params = []\n",
    "    for kk in range(K):\n",
    "        labels = y_train > thresholds[kk]\n",
    "        # ... fit logistic regression to these labels\n",
    "        params.append(fit_logreg_gradopt(X_train, labels, 30))\n",
    "\n",
    "    pred_list_train = []\n",
    "    pred_list_val = []\n",
    "\n",
    "    #sigmoid function to be used to normalise X\n",
    "    def sigmoid_func(x): \n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "    #utilise the parameters from the model fitted earlier and apply sigmoid on the predictions\n",
    "    for i in range(K):\n",
    "        pred_list_train.append(sigmoid_func(X_train @ params[i][0] + params[i][1]))\n",
    "        pred_list_val.append(sigmoid_func(X_val @ params[i][0] + params[i][1]))\n",
    "        \n",
    "    X_train_transform = np.vstack(pred_list_train).T\n",
    "    X_val_transform = np.vstack(pred_list_val).T    \n",
    "\n",
    "    #fit linear regression on the predictions\n",
    "    w_train, b_train = fit_linreg(X_train_transform, y_train, 30)\n",
    "    train_rmse = compute_rmse(X_train_transform, y_train, w_train, b_train)\n",
    "    val_rmse = compute_rmse(X_val_transform, y_val, w_train, b_train)\n",
    "\n",
    "    print(f'RMSE for training linear regression using K = {K}: {train_rmse}')\n",
    "    print(f'RMSE for validation linear regression using K = {K}: {val_rmse}')\n",
    "    k_tunning_results.append((K, train_rmse, val_rmse))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best K value is 14.\n",
      "The training RMSE is 0.15734860991637672 and the validation RMSE is 0.24867525239682137.\n"
     ]
    }
   ],
   "source": [
    "best_k = np.argmin(np.array(k_tunning_results)[:,2])\n",
    "print(f'The best K value is {k_tunning_results[best_k][0]}.')\n",
    "print(f'The training RMSE is {k_tunning_results[best_k][1]} and the validation RMSE is {k_tunning_results[best_k][2]}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for testing linear regression using K = 20: 0.28433088097744263\n"
     ]
    }
   ],
   "source": [
    "K = 20\n",
    "mx = np.max(y_train); mn = np.min(y_train); hh = (mx-mn)/(K+1)\n",
    "thresholds = np.linspace(mn+hh, mx-hh, num=K, endpoint=True)\n",
    "params = []\n",
    "for kk in range(K):\n",
    "    labels = y_train > thresholds[kk]\n",
    "    # ... fit logistic regression to these labels\n",
    "    params.append(fit_logreg_gradopt(X_train, labels, 30))\n",
    "\n",
    "pred_list_train = []\n",
    "pred_list_test = []\n",
    "\n",
    "#sigmoid function to be used to normalise X\n",
    "def sigmoid_func(x): \n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "#utilise the parameters from the model fitted earlier and apply sigmoid on the predictions\n",
    "for i in range(K):\n",
    "    pred_list_train.append(sigmoid_func(X_train @ params[i][0] + params[i][1]))\n",
    "    pred_list_test.append(sigmoid_func(X_test @ params[i][0] + params[i][1]))\n",
    "        \n",
    "X_train_transform = np.vstack(pred_list_train).T\n",
    "X_test_transform = np.vstack(pred_list_test).T    \n",
    "\n",
    "#fit linear regression on the predictions\n",
    "w_train, b_train = fit_linreg(X_train_transform, y_train, 30)\n",
    "test_rmse = compute_rmse(X_test_transform, y_test, w_train, b_train)\n",
    "print(f'RMSE for testing linear regression using K = {K}: {test_rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
